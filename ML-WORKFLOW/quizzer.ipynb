{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6821170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Any\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain.tools import BaseTool\n",
    "from langchain_core.tools import Tool\n",
    "\n",
    "# Updated import for memory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_ollama import ChatOllama\n",
    "from pydantic import BaseModel, Field\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9797fc5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'google.genai' has no attribute 'GenerativeModel'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m genai\n\u001b[32m      3\u001b[39m client = genai.Client(api_key=\u001b[33m\"\u001b[39m\u001b[33mAIzaSyDrPY-M6t2BUQ32WIoKOpxfd2izJkxaJPk\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model = \u001b[43mgenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGenerativeModel\u001b[49m(model_name=\u001b[33m\"\u001b[39m\u001b[33mgemini-pro\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# or gemini-1.5-pro / flash\u001b[39;00m\n\u001b[32m      6\u001b[39m response = model.generate_content(\u001b[33m\"\u001b[39m\u001b[33mTell me a joke about AI.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'google.genai' has no attribute 'GenerativeModel'"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "genai.models\n",
    "client = genai.Client(api_key=\"AIzaSyDrPY-M6t2BUQ32WIoKOpxfd2izJkxaJPk\")\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"gemini-pro\")  # or gemini-1.5-pro / flash\n",
    "response = model.generate_content(\"Tell me a joke about AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed73e1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rushil Misra\\Documents\\projects\\TechPaglu\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the AI cross the road?\n",
      "\n",
      "To optimize its path and minimize its carbon footprint, of course! And also, it heard there was a better dataset on the other side.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai  # âœ… correct import\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyDrPY-M6t2BUQ32WIoKOpxfd2izJkxaJPk\")\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"gemini-2.0-flash\")  # or gemini-1.5-pro / flash\n",
    "response = model.generate_content(\"Tell me a joke about AI.\")\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98154442",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3.2:1b\", temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfb6fd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionGeneratorInput(BaseModel):\n",
    "    topic: str = Field(description=\"Topic for the question\")\n",
    "    difficulty: str = Field(description=\"Difficulty level: easy, medium, or hard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0386c8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationInput(BaseModel):\n",
    "    answer: str = Field(description=\"User's answer to evaluate\")\n",
    "    question: str = Field(description=\"The question that was asked\")\n",
    "    difficulty: str = Field(description=\"Difficulty level of the question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a7ed6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionSelectorInput(BaseModel):\n",
    "    current_difficulty: str = Field(description=\"Current difficulty level\")\n",
    "    eval_score: int = Field(description=\"Current evaluation score\")\n",
    "    question_count: int = Field(description=\"Number of questions asked so far\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db281542",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReportGeneratorInput(BaseModel):\n",
    "    questions : List[str] = Field(description=\"List of all user answers\")\n",
    "    \n",
    "    answers: List[str] = Field(description=\"List of all user answers\")\n",
    "    \n",
    "    eval_scores: List[int] = Field(description=\"List of evaluation scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dadc3167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Question_generator(input_data: QuestionGeneratorInput,additional_prompt = \"\") -> str:\n",
    "    \"\"\"Generate a question based on the topic and difficulty level.\"\"\"\n",
    "    prompt = f\"You are a teacher with expertise in {input_data.topic}. Ask a strictly theoretical {input_data.difficulty} level question.\" + additional_prompt\n",
    "    return llm.invoke(prompt).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82b8598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answer(input_data: EvaluationInput) -> Dict[str, Any]:\n",
    "\t\"\"\"you are a highly educated expert and your job is to perform strict evaluation of the answer based on the given question\"\"\"\n",
    "\t# Simple evaluation logic - can be made more sophisticated\n",
    "\tprompt = f\"\"\"you are a highly educated expert and your job is to perform strict evaluation of the answer based on the given question\"{input_data.question}\" \n",
    "\tAnd the answer: \"{input_data.answer}\"\n",
    "\tEvaluate this {input_data.difficulty} level answer on a scale of 0-10.\n",
    "\tmake sure to respond with a number between 0 and 10.\"\"\"\n",
    "\tscore = llm.invoke(prompt)\n",
    "\ttry:\n",
    "\t\tmatch = re.search(r'\\d+', score.content)\n",
    "\t\tif match:\n",
    "\t\t\t\treturn int(match.group())\n",
    "\texcept:\n",
    "\t\treturn 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ee48ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_next_question(input_data: QuestionSelectorInput) -> Dict[str, Any]:\n",
    "    \"\"\"Choose the next question difficulty based on performance.\"\"\"\n",
    "    curr_type = input_data.current_difficulty\n",
    "    eval_score = input_data.eval_score\n",
    "    question_count = input_data.question_count\n",
    "    \n",
    "    if curr_type == \"easy\" and eval_score >= 5:\n",
    "        new_type = \"medium\"\n",
    "    elif curr_type == \"medium\":\n",
    "        if eval_score >= 8:\n",
    "            new_type = \"hard\"\n",
    "        elif eval_score < 5:\n",
    "            new_type = \"easy\"\n",
    "        else:\n",
    "            new_type = \"medium\"\n",
    "    elif curr_type == \"hard\" and eval_score < 7:\n",
    "        new_type = \"medium\"\n",
    "    else:\n",
    "        new_type = curr_type\n",
    "\n",
    "    should_continue = question_count < 6\n",
    "    \n",
    "    return {\n",
    "        \"new_difficulty\": new_type,\n",
    "        \"should_continue\": should_continue\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b44c9b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_report(input_data: ReportGeneratorInput) -> str:\n",
    "    \"\"\"Generate a final report of the user's performance.\"\"\"\n",
    "    prompt = f\"\"\"Based on these answers and scores, write a short evaluation highlighting strengths, weaknesses, and areas for improvement:\n",
    "    - Average score: {sum(input_data.eval_scores) / len(input_data.eval_scores) if input_data.eval_scores else 0}\n",
    "    \"\"\"\n",
    "    return llm.invoke(prompt).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab4354a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_report(input_data : ReportGeneratorInput) -> str:\n",
    "\t\"\"\"Generate a final report of the user's performance.\"\"\"\n",
    "\tzipped_lists = zip(input_data.questions, input_data.answers, input_data.eval_scores) \n",
    "\tnew_lists = [list(group) for group in zipped_lists]\n",
    "\tprompt = f\"You are highly experienced in evaluation and you have this list : {new_lists} which contains list of [questions,answers,score], you have to create a short report evaluating the student's skills and especially create a list of weaknesses the student needs to work on\"\n",
    "\treturn llm.invoke(prompt).content\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1832032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['topic', 'difficulty'])\n"
     ]
    }
   ],
   "source": [
    "print(QuestionGeneratorInput.model_fields.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b0cdee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ReportGeneratorInput\nquestions\n  Field required [type=missing, input_value={'answers': ['global can ...l_scores': [2, 8, 8, 0]}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m new_difficulty,should_continue = choose_next_question(input_data_qs)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_continue \u001b[38;5;129;01mor\u001b[39;00m flag>\u001b[32m3\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \tinput_data_r = \u001b[43mReportGeneratorInput\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m\t\t\u001b[49m\u001b[43manswers\u001b[49m\u001b[43m=\u001b[49m\u001b[43manswers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m\t\t\u001b[49m\u001b[43meval_scores\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscores\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m\t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \treport = generate_final_report(input_data_r)\n\u001b[32m     48\u001b[39m \t\u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rushil Misra\\Documents\\projects\\TechPaglu\\.venv\\Lib\\site-packages\\pydantic\\main.py:253\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    252\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    255\u001b[39m     warnings.warn(\n\u001b[32m    256\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    259\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    260\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for ReportGeneratorInput\nquestions\n  Field required [type=missing, input_value={'answers': ['global can ...l_scores': [2, 8, 8, 0]}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing"
     ]
    }
   ],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "scores = []\n",
    "\n",
    "difficulty = \"easy\"\n",
    "flag = 1\n",
    "while True:\n",
    "\tinput_data_q = QuestionGeneratorInput(\n",
    "\ttopic = \"python\",\n",
    "\tdifficulty = difficulty        \n",
    "\t)\n",
    "\n",
    "\t\n",
    "\twhile True:\n",
    "\t\tquestion = Question_generator(input_data_q)\n",
    "\t\tif question not in questions:\n",
    "\t\t\tbreak\n",
    "\t\telse:\n",
    "\t\t\tquestion = Question_generator(input_data_q,additional_prompt=\"Generate a new and different question\")\n",
    "\n",
    "\tuserMessage = input(f\"answer the following : {question}\")\n",
    "\n",
    "\tinput_data_e=EvaluationInput(\n",
    "\t\t\t\t\tanswer=userMessage,\n",
    "\t\t\t\t\tquestion=question,\n",
    "\t\t\t\t\tdifficulty=difficulty\n",
    "\t\t\t\t)\n",
    "\t\n",
    "\tscore = int(evaluate_answer(input_data_e))\n",
    "\tquestions.append(question)\n",
    "\tanswers.append(userMessage)\n",
    "\tscores.append(score)\n",
    "\n",
    "\tinput_data_qs = QuestionSelectorInput(\n",
    "\t\tcurrent_difficulty= difficulty,\n",
    "\t\teval_score=score,\n",
    "\t\tquestion_count=flag\n",
    "\t)\n",
    "\n",
    "\tnew_difficulty,should_continue = choose_next_question(input_data_qs)\n",
    "\n",
    "\tif not should_continue or flag>3:\n",
    "\t\tinput_data_r = ReportGeneratorInput(\n",
    "\t\t\tquestions=questions,\n",
    "\t\t\tanswers=answers,\n",
    "\t\t\teval_scores=scores\n",
    "\t\t)\n",
    "\t\treport = generate_final_report(input_data_r)\n",
    "\t\tbreak\n",
    "\telse:\n",
    "\t\tdifficulty = new_difficulty\n",
    "\t\tflag +=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32475cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
