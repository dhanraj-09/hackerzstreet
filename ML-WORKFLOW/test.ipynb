{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499e17ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from typing import Dict, List, Any\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain.tools import BaseTool\n",
    "from langchain_core.tools import Tool\n",
    "\n",
    "# Updated import for memory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_ollama import ChatOllama\n",
    "from pydantic import BaseModel, Field\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470dbbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3.2:1b\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50efba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionGeneratorInput(BaseModel):\n",
    "    language: str = Field(description=\"Programming language for the question\")\n",
    "    field: str = Field(description=\"Technical field for the question\")\n",
    "    dsa: str = Field(description=\"Data structure or algorithm focus\")\n",
    "    difficulty: str = Field(description=\"Difficulty level: easy, medium, or hard\")\n",
    "\n",
    "class EvaluationInput(BaseModel):\n",
    "    answer: str = Field(description=\"User's answer to evaluate\")\n",
    "    question: str = Field(description=\"The question that was asked\")\n",
    "    difficulty: str = Field(description=\"Difficulty level of the question\")\n",
    "\n",
    "class QuestionSelectorInput(BaseModel):\n",
    "    current_difficulty: str = Field(description=\"Current difficulty level\")\n",
    "    eval_score: int = Field(description=\"Current evaluation score\")\n",
    "    question_count: int = Field(description=\"Number of questions asked so far\")\n",
    "\n",
    "class ReportGeneratorInput(BaseModel):\n",
    "    answers: List[str] = Field(description=\"List of all user answers\")\n",
    "    eval_scores: List[int] = Field(description=\"List of evaluation scores\")\n",
    "    difficulty_counts: Dict[str, int] = Field(description=\"Count of questions by difficulty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dcafaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_generator(input_data: QuestionGeneratorInput) -> str:\n",
    "    \"\"\"Generate a question based on language, field, DSA concept and difficulty level.\"\"\"\n",
    "    prompt = f\"You are a teacher with expertise in {input_data.language}, {input_data.field}, and {input_data.dsa}. Ask a {input_data.difficulty} level question.\"\n",
    "    return llm.invoke(prompt).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de36aa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answer(input_data: EvaluationInput) -> Dict[str, Any]:\n",
    "\t\"\"\"you are a highly educated expert and your job is to perform strict evaluation of the answer based on the given question\"\"\"\n",
    "\t# Simple evaluation logic - can be made more sophisticated\n",
    "\tprompt = f\"\"\"you are a highly educated expert and your job is to perform strict evaluation of the answer based on the given question\"{input_data.question}\" \n",
    "\tAnd the answer: \"{input_data.answer}\"\n",
    "\tEvaluate this {input_data.difficulty} level answer on a scale of 0-10.\n",
    "\tmake sure to respond with a number between 0 and 10.\"\"\"\n",
    "\tscore = llm.invoke(prompt)\n",
    "\ttry:\n",
    "\t\tmatch = re.search(r'\\d+', score.content)\n",
    "\t\tif match:\n",
    "\t\t\t\treturn int(match.group())\n",
    "\texcept:\n",
    "\t\treturn 5\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55becd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_next_question(input_data: QuestionSelectorInput) -> Dict[str, Any]:\n",
    "    \"\"\"Choose the next question difficulty based on performance.\"\"\"\n",
    "    curr_type = input_data.current_difficulty\n",
    "    eval_score = input_data.eval_score\n",
    "    question_count = input_data.question_count\n",
    "    \n",
    "    if curr_type == \"easy\" and eval_score >= 5:\n",
    "        new_type = \"medium\"\n",
    "    elif curr_type == \"medium\":\n",
    "        if eval_score >= 8:\n",
    "            new_type = \"hard\"\n",
    "        elif eval_score < 5:\n",
    "            new_type = \"easy\"\n",
    "        else:\n",
    "            new_type = \"medium\"\n",
    "    elif curr_type == \"hard\" and eval_score < 7:\n",
    "        new_type = \"medium\"\n",
    "    else:\n",
    "        new_type = curr_type\n",
    "\n",
    "    should_continue = question_count < 6\n",
    "    \n",
    "    return {\n",
    "        \"new_difficulty\": new_type,\n",
    "        \"should_continue\": should_continue\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c53ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_report(input_data: ReportGeneratorInput) -> str:\n",
    "    \"\"\"Generate a final report of the user's performance.\"\"\"\n",
    "    prompt = f\"\"\"Based on these answers and scores, write a short evaluation highlighting strengths, weaknesses, and areas for improvement:\n",
    "    - Number of easy questions answered: {input_data.difficulty_counts.get('easy', 0)}\n",
    "    - Number of medium questions answered: {input_data.difficulty_counts.get('medium', 0)}\n",
    "    - Number of hard questions answered: {input_data.difficulty_counts.get('hard', 0)}\n",
    "    - Average score: {sum(input_data.eval_scores) / len(input_data.eval_scores) if input_data.eval_scores else 0}\n",
    "    \"\"\"\n",
    "    return llm.invoke(prompt).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddff0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an adaptive quiz agent. Your job is to:\n",
    "1. Ask questions about programming topics based on language, field, and data structures/algorithms\n",
    "2. Evaluate user answers\n",
    "3. Adjust difficulty based on performance\n",
    "4. Generate a final report after 15 questions\n",
    "\n",
    "Follow this process:\n",
    "1. Start with an easy question about the specified language, field, and DSA concept\n",
    "2. After receiving an answer, evaluate it\n",
    "3. Determine the next question's difficulty based on performance\n",
    "4. After 15 questions, generate a final report\n",
    "\n",
    "Track the following:\n",
    "- Current difficulty level (starts as \"easy\")\n",
    "- Total evaluation score\n",
    "- Question count\n",
    "- Number of easy/medium/hard questions answered\n",
    "- List of all answers and their evaluation scores\n",
    "\n",
    "When generating the final report, analyze the user's performance and provide specific feedback.\n",
    "You have to ask the questions and evaluate answers\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba586a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph,START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class chatstate(TypedDict):\n",
    "\tmessages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph = StateGraph(chatstate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d339b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_quiz(i):\n",
    "    if i<15:\n",
    "        return \"next question\"\n",
    "    else:\n",
    "        return \"create report\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64516ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.add_node(\"question_generator\",question_generator)\n",
    "graph.add_node(\"evaluate_answer\",evaluate_answer)\n",
    "graph.add_node(\"choose_next_question\",choose_next_question)\n",
    "graph.add_node(\"generate_final_report\",generate_final_report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa8070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph.add_edge(START,\"question_generator\")\n",
    "graph.add_edge(\"question_generator\",\"evaluate_answer\")\n",
    "graph.add_edge(\"evaluate_answer\",\"choose_next_question\")\n",
    "# graph.add_edge(\"choose_next_question\",\"question_generator\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0df299",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.add_conditional_edges(\"choose_next_question\",\n",
    "end_quiz,{\n",
    "\"next question\": \"question_generator\",\n",
    "\"create report\" : \"generate_final_report\"\n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cf50b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27a4676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(builder.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d34be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f2ef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified version of your code with fixes for the graph invocation\n",
    "\n",
    "# First, modify your existing functions to work with the message state:\n",
    "def question_generator_node(state):\n",
    "    # Extract necessary information from state or use defaults\n",
    "    language = \"Python\"  # Default or extract from state\n",
    "    field = \"Software Engineering\"  # Default or extract from state\n",
    "    dsa = \"Algorithm Design\"  # Default or extract from state  \n",
    "    difficulty = \"easy\"  # Start with easy\n",
    "    \n",
    "    # Create input data\n",
    "    input_data = QuestionGeneratorInput(\n",
    "        language=language,\n",
    "        field=field,\n",
    "        dsa=dsa,\n",
    "        difficulty=difficulty\n",
    "    )\n",
    "    \n",
    "    # Call the actual function\n",
    "    question = question_generator(input_data)\n",
    "    \n",
    "    # Add the generated question to the messages\n",
    "    state[\"messages\"].append({\"role\": \"assistant\", \"content\": question})\n",
    "    state[\"question\"] = question\n",
    "    state[\"difficulty\"] = difficulty\n",
    "    state[\"question_count\"] = 1\n",
    "    \n",
    "    return state\n",
    "\n",
    "def evaluate_answer_node(state):\n",
    "    # Extract the answer from user input\n",
    "    user_messages = [m for m in state[\"messages\"] if m.get(\"role\") == \"user\"]\n",
    "    if not user_messages:\n",
    "        # No user response yet\n",
    "        return state\n",
    "    \n",
    "    answer = user_messages[-1][\"content\"]\n",
    "    question = state.get(\"question\", \"\")\n",
    "    difficulty = state.get(\"difficulty\", \"easy\")\n",
    "    \n",
    "    # Create input data\n",
    "    input_data = EvaluationInput(\n",
    "        answer=answer,\n",
    "        question=question,\n",
    "        difficulty=difficulty\n",
    "    )\n",
    "    \n",
    "    # Get the evaluation score\n",
    "    result = evaluate_answer(input_data)\n",
    "    score = result  # Assuming your evaluate_answer returns a score\n",
    "    \n",
    "    # Add to state\n",
    "    if \"eval_scores\" not in state:\n",
    "        state[\"eval_scores\"] = []\n",
    "    state[\"eval_scores\"].append(score)\n",
    "    state[\"current_score\"] = score\n",
    "    \n",
    "    # Add evaluation message\n",
    "    state[\"messages\"].append({\"role\": \"assistant\", \"content\": f\"Your answer scored {score}/10\"})\n",
    "    \n",
    "    return state\n",
    "\n",
    "def choose_next_question_node(state):\n",
    "    # Get current state\n",
    "    current_difficulty = state.get(\"difficulty\", \"easy\")\n",
    "    eval_score = state.get(\"current_score\", 5)\n",
    "    question_count = state.get(\"question_count\", 1)\n",
    "    \n",
    "    # Create input data\n",
    "    input_data = QuestionSelectorInput(\n",
    "        current_difficulty=current_difficulty,\n",
    "        eval_score=eval_score,\n",
    "        question_count=question_count\n",
    "    )\n",
    "    \n",
    "    # Choose next question\n",
    "    result = choose_next_question(input_data)\n",
    "    new_difficulty = result[\"new_difficulty\"]\n",
    "    should_continue = result[\"should_continue\"]\n",
    "    \n",
    "    # Update state\n",
    "    state[\"difficulty\"] = new_difficulty\n",
    "    state[\"should_continue\"] = should_continue\n",
    "    state[\"question_count\"] = question_count + 1\n",
    "    \n",
    "    # Track difficulty counts\n",
    "    if \"difficulty_counts\" not in state:\n",
    "        state[\"difficulty_counts\"] = {\"easy\": 0, \"medium\": 0, \"hard\": 0}\n",
    "    state[\"difficulty_counts\"][current_difficulty] = state[\"difficulty_counts\"].get(current_difficulty, 0) + 1\n",
    "    \n",
    "    return state\n",
    "\n",
    "def generate_final_report_node(state):\n",
    "    # Get data from state\n",
    "    eval_scores = state.get(\"eval_scores\", [])\n",
    "    difficulty_counts = state.get(\"difficulty_counts\", {\"easy\": 0, \"medium\": 0, \"hard\": 0})\n",
    "    \n",
    "    # Extract all user answers\n",
    "    answers = [m[\"content\"] for m in state[\"messages\"] if m.get(\"role\") == \"user\"]\n",
    "    \n",
    "    # Create input data\n",
    "    input_data = ReportGeneratorInput(\n",
    "        answers=answers,\n",
    "        eval_scores=eval_scores,\n",
    "        difficulty_counts=difficulty_counts\n",
    "    )\n",
    "    \n",
    "    # Generate the report\n",
    "    report = generate_final_report(input_data)\n",
    "    \n",
    "    # Add to state\n",
    "    state[\"messages\"].append({\"role\": \"assistant\", \"content\": f\"## Final Report\\n\\n{report}\"})\n",
    "    state[\"final_report\"] = report\n",
    "    \n",
    "    return state\n",
    "\n",
    "def end_quiz_condition(state):\n",
    "    question_count = state.get(\"question_count\", 0)\n",
    "    if question_count < 15:\n",
    "        return \"next_question\"\n",
    "    else:\n",
    "        return \"create_report\"\n",
    "\n",
    "# Recreate the graph with the modified nodes\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class chatstate(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    question: str\n",
    "    difficulty: str\n",
    "    question_count: int\n",
    "    current_score: int\n",
    "    eval_scores: list\n",
    "    difficulty_counts: dict\n",
    "    should_continue: bool\n",
    "    final_report: str\n",
    "\n",
    "# Create the graph\n",
    "graph = StateGraph(chatstate)\n",
    "\n",
    "# Add nodes\n",
    "graph.add_node(\"question_generator\", question_generator_node)\n",
    "graph.add_node(\"evaluate_answer\", evaluate_answer_node)\n",
    "graph.add_node(\"choose_next_question\", choose_next_question_node)\n",
    "graph.add_node(\"generate_final_report\", generate_final_report_node)\n",
    "\n",
    "# Add edges\n",
    "graph.add_edge(START, \"question_generator\")\n",
    "graph.add_edge(\"question_generator\", \"evaluate_answer\")\n",
    "graph.add_edge(\"evaluate_answer\", \"choose_next_question\")\n",
    "graph.add_conditional_edges(\n",
    "    \"choose_next_question\",\n",
    "    end_quiz_condition,\n",
    "    {\n",
    "        \"next_question\": \"question_generator\",\n",
    "        \"create_report\": \"generate_final_report\"\n",
    "    }\n",
    ")\n",
    "graph.add_edge(\"generate_final_report\", END)\n",
    "\n",
    "# Compile the graph\n",
    "builder = graph.compile()\n",
    "\n",
    "# Now invoke the graph with proper initial state\n",
    "initial_state = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are an adaptive quiz agent that generates programming questions.\"},\n",
    "        {\"role\": \"user\", \"content\": \"I want to practice Python programming questions about data structures.\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# The correct way to invoke the graph:\n",
    "# result = builder.invoke(initial_state)\n",
    "print(\"Now you can run: result = builder.invoke(initial_state)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95ea7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = builder.invoke(initial_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081579ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
