{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee61acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph,START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b74faf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(model = \"llama3.2:1b\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b007dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class chatstate(TypedDict):\n",
    "    question: Annotated[list, add_messages]\n",
    "    answer: Annotated[list, add_messages]\n",
    "    easy: int\n",
    "    medium: int\n",
    "    hard: int\n",
    "    curr_question_type: str\n",
    "    eval_score: int\n",
    "    question_count: int\n",
    "    final_report: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc5d0015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_bot(language, field, DSA, difficulty=\"easy\"):\n",
    "    prompt = f\"You are a teacher with expertise in {language}, {field}, and {DSA}. Ask a {difficulty} level question.\"\n",
    "    return {\"questions\": [llm.invoke(prompt)]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d22ef0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_question(curr_question_type, eval_score, question_count):\n",
    "    if curr_question_type == \"easy\" and eval_score >= 5:\n",
    "        new_type = \"medium\"\n",
    "    elif curr_question_type == \"medium\":\n",
    "        if eval_score >= 8:\n",
    "            new_type = \"hard\"\n",
    "        elif eval_score < 5:\n",
    "            new_type = \"easy\"\n",
    "        else:\n",
    "            new_type = \"medium\"\n",
    "    elif curr_question_type == \"hard\" and eval_score < 7:\n",
    "        new_type = \"medium\"\n",
    "    else:\n",
    "        new_type = curr_question_type\n",
    "\n",
    "    question_count += 1\n",
    "    return {\n",
    "        \"curr_question_type\": new_type,\n",
    "        \"question_count\": question_count\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b367c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(question_count):\n",
    "    if question_count < 15:\n",
    "        return \"next_question\"\n",
    "    else:\n",
    "        return \"generate_report\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5b037e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report(state):\n",
    "    # Summarize user's performance\n",
    "    prompt = f\"Based on these answers and scores, write a short evaluation highlighting strengths, weaknesses, and areas for improvement.\"\n",
    "    combined_data = f\"Answers: {state['answer']}, Eval Scores: {state['eval_score']}\"\n",
    "    report = llm.invoke(prompt + combined_data)\n",
    "    return {\"final_report\": report}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df9791f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(state):\n",
    "    answer = state[\"answer\"][-1]  # Get the latest answer\n",
    "    question_type = state[\"curr_question_type\"]\n",
    "    eval_score = state[\"eval_score\"]\n",
    "\n",
    "    # For simplicity, let's simulate the evaluation as a correctness check\n",
    "    # You can later plug in a tool/LLM to verify answer quality more deeply\n",
    "    if \"correct\" in answer.lower():  # Simulated evaluation rule\n",
    "        eval_score += 1\n",
    "        if question_type == \"easy\":\n",
    "            state[\"easy\"] += 1\n",
    "        elif question_type == \"medium\":\n",
    "            state[\"medium\"] += 1\n",
    "        elif question_type == \"hard\":\n",
    "            state[\"hard\"] += 1\n",
    "    else:\n",
    "        eval_score += 0  # No change if wrong (can also subtract if desired)\n",
    "\n",
    "    return {\n",
    "        \"eval_score\": eval_score,\n",
    "        \"easy\": state[\"easy\"],\n",
    "        \"medium\": state[\"medium\"],\n",
    "        \"hard\": state[\"hard\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3631a732",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m builder.add_edge(\u001b[33m\"\u001b[39m\u001b[33mchoosing question\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontinue?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# âœ… Now: Conditional edges from a registered node to other registered nodes\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_conditional_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontinue?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnext_question\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestioning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgenerate_report\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgenerate_report\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     22\u001b[39m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rushil Misra\\Documents\\projects\\TechPaglu\\.venv\\Lib\\site-packages\\langgraph\\graph\\state.py:525\u001b[39m, in \u001b[36mStateGraph.add_conditional_edges\u001b[39m\u001b[34m(self, source, path, path_map, then)\u001b[39m\n\u001b[32m    519\u001b[39m     logger.warning(\n\u001b[32m    520\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAdding an edge to a graph that has already been compiled. This will \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    521\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mnot be reflected in the compiled graph.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    522\u001b[39m     )\n\u001b[32m    524\u001b[39m \u001b[38;5;66;03m# find a name for the condition\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m path = \u001b[43mcoerce_to_runnable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    526\u001b[39m name = path.name \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcondition\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    527\u001b[39m \u001b[38;5;66;03m# validate the condition\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rushil Misra\\Documents\\projects\\TechPaglu\\.venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py:493\u001b[39m, in \u001b[36mcoerce_to_runnable\u001b[39m\u001b[34m(thing, name, trace)\u001b[39m\n\u001b[32m    486\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m RunnableCallable(\n\u001b[32m    487\u001b[39m             thing,\n\u001b[32m    488\u001b[39m             wraps(thing)(partial(run_in_executor, \u001b[38;5;28;01mNone\u001b[39;00m, thing)),  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    489\u001b[39m             name=name,\n\u001b[32m    490\u001b[39m             trace=trace,\n\u001b[32m    491\u001b[39m         )\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(thing, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRunnableParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    495\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    496\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected a Runnable, callable or dict.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    497\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInstead got an unsupported type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(thing)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    498\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rushil Misra\\Documents\\projects\\TechPaglu\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3581\u001b[39m, in \u001b[36mRunnableParallel.__init__\u001b[39m\u001b[34m(self, steps__, **kwargs)\u001b[39m\n\u001b[32m   3578\u001b[39m merged = {**steps__} \u001b[38;5;28;01mif\u001b[39;00m steps__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m   3579\u001b[39m merged.update(kwargs)\n\u001b[32m   3580\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3581\u001b[39m     steps__={key: \u001b[43mcoerce_to_runnable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, r \u001b[38;5;129;01min\u001b[39;00m merged.items()}\n\u001b[32m   3582\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rushil Misra\\Documents\\projects\\TechPaglu\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5940\u001b[39m, in \u001b[36mcoerce_to_runnable\u001b[39m\u001b[34m(thing)\u001b[39m\n\u001b[32m   5935\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[33m\"\u001b[39m\u001b[33mRunnable[Input, Output]\u001b[39m\u001b[33m\"\u001b[39m, RunnableParallel(thing))\n\u001b[32m   5936\u001b[39m msg = (\n\u001b[32m   5937\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected a Runnable, callable or dict.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5938\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInstead got an unsupported type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(thing)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   5939\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m5940\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[31mTypeError\u001b[39m: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'str'>"
     ]
    }
   ],
   "source": [
    "builder = StateGraph(chatstate)\n",
    "\n",
    "# 1. Register ALL nodes\n",
    "builder.add_node(\"questioning\", question_bot)\n",
    "builder.add_node(\"evaluate\", evaluate)\n",
    "builder.add_node(\"choosing question\", choose_question)\n",
    "builder.add_node(\"continue?\", should_continue)\n",
    "builder.add_node(\"generate_report\", generate_report)\n",
    "\n",
    "# 2. Set entry point\n",
    "builder.set_entry_point(\"questioning\")\n",
    "\n",
    "# 3. Add edges\n",
    "builder.add_edge(\"questioning\", \"evaluate\")\n",
    "builder.add_edge(\"evaluate\", \"choosing question\")\n",
    "builder.add_edge(\"choosing question\", \"continue?\")\n",
    "\n",
    "# âœ… Now: Conditional edges from a registered node to other registered nodes\n",
    "builder.add_conditional_edges(\"continue?\", {\n",
    "    \"next_question\": \"questioning\",\n",
    "    \"generate_report\": \"generate_report\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc2b006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
